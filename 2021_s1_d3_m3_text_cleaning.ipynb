{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021_s1_d3_m3a_text_cleaning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZRkKMCKNkvdi"},"source":["# I.  Text Cleaning and Preparation:  Introduction"]},{"cell_type":"markdown","metadata":{"id":"RScxV--UlCxb"},"source":["Some basic code for text cleaning and prep"]},{"cell_type":"markdown","metadata":{"id":"VrayDq7yqKOr"},"source":["# II.  Setup the Environment"]},{"cell_type":"markdown","metadata":{"id":"gLc9k4lCqYMn"},"source":["We'll focus on usingthe NLTK package to clean and process our text for final analysis.  Comments in the code identify each of the packages being loaded.  In each case, you can refer to the package documentation for more specific information about the package being used.  You must run the code cells below to properly prepare your environment to perfrom the text mining and analysis tasks presented in this module."]},{"cell_type":"code","metadata":{"id":"ZGDtzniRIEix"},"source":["# update collab environment to latest version of NLTK\n","# documentation: https://www.nltk.org/\n","!pip install nltk -U"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a612AzoBqW6B"},"source":["# update to the latest version fo Spacy\n","# https://spacy.io/\n","!pip install spacy -U"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvMcLYrzWZ67"},"source":["# import the base nltk package\n","import nltk\n","\n","# load the nltk tokenize module\n","from nltk.tokenize import word_tokenize\n","\n","# download the punkt model\n","nltk.download('punkt')\n","\n","# import nltk stopword module\n","from nltk.corpus import stopwords\n","\n","# donload the stopword list\n","nltk.download('stopwords')\n","\n","# import the nltk porter stemmer\n","from nltk.stem.porter import PorterStemmer\n","\n","# import the nltk lemmatizer\n","from nltk.stem import WordNetLemmatizer\n","\n","# download nltk wordnet model\n","nltk.download('wordnet')\n","\n","# import the regular expressions package\n","import re\n","\n","# inport the string package\n","import string\n","\n","# import Spacy\n","import spacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkUWCW8IrbSZ"},"source":["# download and install the spacy language model\n","!python -m spacy download en_core_web_sm\n","sp=spacy.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qItWOMNKtu2S"},"source":["# III.  Load the Text File"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5m5fdnh2vAQW","executionInfo":{"status":"ok","timestamp":1626242334159,"user_tz":420,"elapsed":26404,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}},"outputId":"f798f975-12b4-45c4-bf61-3c76d88e72e2"},"source":["from google.colab import drive\n","drive.mount('/gdrive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"enKrUpF4wZwm"},"source":["working_file_path = \"/gdrive/MyDrive/rbs_digital_approaches_2021/data_class/melville.txt\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWwLexukweVo"},"source":["Now that you've defined a file to load, we can open the file and read its contents into a string variable."]},{"cell_type":"code","metadata":{"id":"VNRlMPS1Wzi3"},"source":["# open a text file for processing\n","working_file = open(working_file_path, \"r\")\n","\n","# read the file contents into a string variable\n","working_text = working_file.read()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WHlWPWmZxlNC"},"source":["You can check that your file loaded by checking the length and examining the opening characters of the working_text variable."]},{"cell_type":"code","metadata":{"id":"pzNkffHdx3us"},"source":["# print the character length of our working text\n","# and the first several characters\n","print('Characters in string:', len(working_text))\n","print(working_text[:600:1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cuBJlhl4mgBh"},"source":["IV.  Convert to Lowercase"]},{"cell_type":"markdown","metadata":{"id":"yS3b7yZsl9WN"},"source":["# IV.  Remove Newline Characters and Strip Spaces"]},{"cell_type":"code","metadata":{"id":"T9bwg0lcmFli"},"source":["# define a pattern for finding newlines\n","pattern = re.compile(r\"\\n\", re.DOTALL | re.MULTILINE | re.IGNORECASE)\n","# run the replacement.  \n","working_text = re.sub(pattern, \" \", working_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYW_tB6HsCfI"},"source":["# define a patern for finding multiple spaces\n","pattern = re.compile(r\"\\s+\")\n","# run the replacement\n","working_text = re.sub(pattern, \" \", working_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-stuZMLxrfp_"},"source":["# strip leading and trailing spaces\n","working_text = working_text.strip()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y51Y1tFaMkZ1"},"source":["# IV. Remove Editorial Text"]},{"cell_type":"code","metadata":{"id":"wXSy46brM0Zy"},"source":["# look at the first 600 characters of the string\n","print(len(working_text))\n","print(working_text[:600:1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJpSnomdQgrd"},"source":["# define a pattern and remove the opening text\n","pattern = re.compile(r\"^.*chapter 1\\. Loomings\\.?\", re.IGNORECASE)\n","working_text = re.sub(pattern, \"\", working_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8qn5hHMgz2wm"},"source":["# define a pattern and remove the closing text\n","pattern = re.compile(r\"End of Project Gutenberg's.*\", re.IGNORECASE)\n","working_text = re.sub(pattern, \"\", working_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTmux5QznszI"},"source":["# look at the first 600 characters of the string\n","print(len(working_text))\n","print(working_text[1190000:1190046:1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3s8Jk73t16X0"},"source":["# V. Save Clean Blob Version"]},{"cell_type":"markdown","metadata":{"id":"Kjirbci9QVRX"},"source":["It's a good idea to put aside a version of the minimally claeaned text as single blob for use later. "]},{"cell_type":"code","metadata":{"id":"X4x4G0Nx2DFJ"},"source":["blob_text = working_text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2JZkHW0l5zws"},"source":["# For the rest of the cleaning we'll tokenize and then clean because NLTK Likes it that way"]},{"cell_type":"code","metadata":{"id":"7_vKy4c95-9H"},"source":["# tokenize on words\n","tokens = word_tokenize(working_text)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTMuWbEB8ip_"},"source":["# look at the results\n","print(tokens[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9jfxpoG8PD-"},"source":["# remove punctuation from each word\n","table = str.maketrans('', '', string.punctuation)\n","filtered_tokens = [w.translate(table) for w in tokens]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mg0CPx7E9IVS"},"source":["# look at the results\n","print(filtered_tokens[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-bJFaPp9IHP"},"source":["# remove remaining tokens that are not alphabetic\n","filtered_tokens = [word for word in filtered_tokens if word.isalpha()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Cy8M7Gr90Ak"},"source":["# look at the results\n","print(filtered_tokens[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkEzXADI9zZq"},"source":["# load the nltp stopword list\n","stop_words = set(stopwords.words('english'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eXjxqA-B-xI4"},"source":["# review the stopwords\n","print(stop_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cWa4zTlfB0c8"},"source":["Note that based on your research qustion you might want to modify the stopword list.  You can reuse code from above (for removing spaces, etc.) to create a list of words you want to remoove from the stopword list.  Alternatively, you can append other words to this list or build your own from scratch."]},{"cell_type":"code","metadata":{"id":"hlBnhX4A-zUc"},"source":["# remove the stopwords\n","filtered_tokens = [w for w in filtered_tokens if not w in stop_words]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZMbsYD6Ay6i"},"source":["# look at the results\n","print(filtered_tokens[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6gU6fkWBnY1"},"source":["# Stemming\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GMynaab7RGbe"},"source":["Stemmking uses a rules-based algorithm to remove plural endings, \"ing\" endings and the like from words as a means of reducing variation.  [See the Wikipedia article here for a good overview](https://en.wikipedia.org/wiki/Stemming)."]},{"cell_type":"code","metadata":{"id":"_mWtdD8hCKp3"},"source":["# instantiate a porter stemmer class object\n","p_stemmer = PorterStemmer()\n","\n","#  run the stemmer on our list of filtered words\n","stemmed_tokens = [p_stemmer.stem(word) for word in filtered_tokens]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cXLeUMBPC9P8"},"source":["# view the results\n","print(stemmed_tokens[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qnf8mMY9q390"},"source":["# Lemmatizing"]},{"cell_type":"markdown","metadata":{"id":"pFsMju-DRpVS"},"source":["Lemmatization is an NLP based reduction method that uses language models to reduce all variants of word (is, are, am, etc.) to a common linguistic root (be).  It generally improves the quality of semantic models because it reduces lexical variation in favor of semantic sameness.  However, in many cases we care a lot about particular words, and stemming often erases these differences, so think carefully before stemming."]},{"cell_type":"code","metadata":{"id":"cpx6hXiyq89x"},"source":["sp_text = sp(blob_text[:100:1])\n","for word in sp_text:\n","  print(word.text, word.lemma, word.lemma_)\n"],"execution_count":null,"outputs":[]}]}